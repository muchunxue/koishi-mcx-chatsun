# koishi-mcx-chatsun基本介绍

chatsun一款基于 Langchain 框架构建的角色扮演 AI 对话插件。chatsun将针对“真实的角色扮演”这一目标进行设计，具体来说，chatsun将一个角色视为一个单独的个体，chatsun的所有功能模块都是为了强化该个体的能力上限，为此，

## chatsun将提供以下功能模块：

### 系统预设
系统用于限制ai的回复模式,破限，文风等设定都在此实现，您可以在 **koishi-app/external/chatsun/src/langchain/prompt/presets** 中新建 TXT 文件并编写您的预设，一个角色只能启用一个预设文件

### 人物预设
人物预设用于定义角色的人设，在 **koishi-app/external/chatsun/src/langchain/prompt/character**中新建 TXT 文件并编写您的人设文件，一个角色只能启用一个人设文件

### 知识库
知识库是一个用于存储大量信息的重要功能。如果将所有内容都交给 AI 处理，不仅会占用大量 Token，效果也可能不尽如人意。因此，在遇到大量资料时，建议将其存放于知识库中。当用户提问与知识库中的内容相关时，插件会自动从知识库提取相关信息，并将其作为 Prompt 的一部分传递给 AI，从而使 AI 更准确地理解问题，在 **koishi-app/external/chatsun/src/langchain/prompt/knowledge** 中新建知识库文件夹，并在知识库文件夹内新建txt文件来编写知识库，知识库文件夹内允许创建多个txt文件。一个角色只能启用一个知识库文件夹。

### 角色属性独立机制
每一个角色都拥有独立的属性，比如人设，预设，知识库，记忆，未来还可能会有技能等，不论在什么地方，角色的属性不变，比如你在私聊告诉了角色你的名字，转而去另外一个私聊或者频道和他对话，会发现他仍然记得你的名字。总而言之，在所有场景下，同一个角色的属性是完全相同的。



# chatsun版本更新日志
目前chatsun仍在开发中，开发日志暂时放在此处，源码暂时不会更新，预计在1.0版本之后一次性放上去。

### 0.0.1：将对话的基础模块构建完成，不过基本不可用，bug一大堆。
### 0.0.2：修复了生产环境下预设文件位置错误的问题，优化了一些小bug。
### 0.0.3：修复了已知bug，优化程序结构，实现角色属性独立机制，当前版本初步具备可用性。






# chatsun配置项说明
关于chatsun的各项配置在koishi内可能没办法完全表述清楚，所以在这里对配置进行详细说明。

### 默认配置
在koishi内的配置是默认角色的配置项，当你在某个群聊或者私聊从未引入角色，将会分配这位默认角色，**当前版本还只能使用默认角色。**

### 角色名称
角色名称是角色的唯一标识，在未来当我们需要某个角色的时候，将会通过角色名称来找到她，**当前版本还未提供创建更多角色的功能，预计在0.0.4版本实现**

### 默认角色人设/预设文件名称
通过填入文件名称来载入角色人设和预设文件，只需要名称，不需要后缀，**当前版本不提供默认的人设，预设文件，需要用户手动创建并放在文件夹内，具体位置见功能模块说明**

### 知识库名称
一个知识库是以一个文件夹为单位的，载入一个知识库将会将这个知识库文件夹内所有文档向量化，**你需要在指定位置自建知识库文件夹，并将你想要存入知识库的所有文件放在该文件夹内，具体位置见功能模块说明**

### 对话历史记录/角色记忆的最大长度
该长度以tokens为单位，硬要说的话，大概一个汉字是1.2tokens左右? 拉高该值可以让ai保留更久远的记忆，但是随着记忆增多，token的消耗将会越来越多，模型价格比较贵的话，谨慎拉高哦。**当角色的的记忆的大小超过该值的时候，将会进行裁剪，将最久远的记忆裁剪掉，以保证tokens不超预期值。** **关于未来————之后可能会提供将记忆存入向量数据库的功能，以实现在一定程度上节省token的同时保留长期记忆**

### 是否启用知识库
启动该选项之后才会启用知识库，但你仍然可以对知识库进行配置，只不过ai在对话的时候不会使用知识库，也不会将知识库向量化。

### 语言模型api地址/apikey/模型名称
api地址填写你的模型的请求地址，记得在请求地址要写到/v1,apikey是你请求模型所需要的key，模型名称是你所请求的模型的名称。

### 平台
平台是指你的模型的供应商，比如chatgpt就所属openai，claude就属于anthropic，gemini模型属于google，值得注意的是custom是符合openai格式的中转站请求方式，基本上中转站都是这个格式。**如果你使用的是官方的api和key，那么你可能不需要填写请求地址，只需要填写key，并且你需要保证运行该插件的设备可以对官方地址进行请求，目前官方的方式我还没有测试过，如果有问题欢迎反映，不过个人还是建议使用中转。**

### 温度
模型温度越高，回答的随机性就越高。

### 嵌入模型api地址/apikey/模型名称/平台
填写方式和语言模型一样。**建议使用默认的模型，别的嵌入模型可能还没做适配，而且别的嵌入模型效果大概也不会比默认的这个好多少**

### 文本向量化批处理大小
知识库向量化的过程是先将需要向量化的文本分开，再一段一段的向量化并载入内存，调大该值理论上可能会提示向量化的速度，但会增大内存负载，非特殊情况，一般建议不要修改。

### 文档分割大小
知识库在向量化之前，还需要提前进行一次分割，将大文档分割成小文档，最终在向量化后的知识库内搜索知识的时候，会将匹配到的小文档整个拿出来放进配置项中，所**以当该值越大，匹配到的文本语义就越完整，tokens消耗越大，越小精度就越高，tokens越小，建议根据知识库的不同来配置大小，通常该值和selectSize的值会配合使用。**

### 分割文档重叠大小
文档分割后，为了保证小文档之间语义的完整，会重叠一部分内容，**通常该值设定为文档分割大小的百分之10到百分之20**

### 知识库单次匹配文档数/selectSize
知识库在搜索知识之后，可以返回多个相关的文档，通常可以降低文档分割大小，再提高匹配文档数来获取精确的所需的知识，具体怎么配置还是建议实践出真知。

### 知识库相似度阈值
知识库在搜索知识之后，会对搜索到的内容进行一次评分，只返回相似度高于该值的文档，拦截低于该值的文档。

